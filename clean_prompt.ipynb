{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /users/klee12/.local/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: transformers in /users/klee12/.local/lib/python3.10/site-packages (4.39.3)\n",
      "Requirement already satisfied: filelock in /users/klee12/.local/lib/python3.10/site-packages (from datasets) (3.13.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /users/klee12/.local/lib/python3.10/site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: multiprocess in /users/klee12/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /users/klee12/.local/lib/python3.10/site-packages (from datasets) (4.66.2)\n",
      "Requirement already satisfied: packaging in /users/klee12/.local/lib/python3.10/site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: xxhash in /users/klee12/.local/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /users/klee12/.local/lib/python3.10/site-packages (from datasets) (0.22.2)\n",
      "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /users/klee12/.local/lib/python3.10/site-packages (from datasets) (2024.2.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /users/klee12/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pyarrow-hotfix in /users/klee12/.local/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: pandas in /users/klee12/.local/lib/python3.10/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /users/klee12/.local/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /users/klee12/.local/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: aiohttp in /users/klee12/.local/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /users/klee12/.local/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /users/klee12/.local/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /users/klee12/.local/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /users/klee12/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /users/klee12/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /users/klee12/.local/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /users/klee12/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /users/klee12/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /users/klee12/.local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /users/klee12/.local/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /users/klee12/.local/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /users/klee12/.local/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /users/klee12/.local/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /users/klee12/.local/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /users/klee12/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /users/klee12/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /users/klee12/.local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/klee12/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /users/klee12/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import interpreter_login\n",
    "\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Downloading readme: 100%|██████████| 1.15k/1.15k [00:00<00:00, 5.98MB/s]\n",
      "Downloading data: 100%|██████████| 79.0M/79.0M [00:01<00:00, 52.4MB/s]\n",
      "Downloading data: 100%|██████████| 80.0M/80.0M [00:01<00:00, 78.1MB/s]\n",
      "Downloading data: 100%|██████████| 78.6M/78.6M [00:01<00:00, 69.2MB/s]\n",
      "Downloading data: 100%|██████████| 73.6M/73.6M [00:00<00:00, 74.6MB/s]\n",
      "Downloading data: 100%|██████████| 79.6M/79.6M [00:01<00:00, 62.4MB/s]\n",
      "Downloading data: 100%|██████████| 77.5M/77.5M [00:00<00:00, 77.6MB/s]\n",
      "Downloading data: 100%|██████████| 79.1M/79.1M [00:01<00:00, 76.6MB/s]\n",
      "Downloading data: 100%|██████████| 83.1M/83.1M [00:01<00:00, 73.8MB/s]\n",
      "Downloading data: 100%|██████████| 77.7M/77.7M [00:01<00:00, 72.3MB/s]\n",
      "Downloading data: 100%|██████████| 82.5M/82.5M [00:01<00:00, 75.4MB/s]\n",
      "Downloading data: 100%|██████████| 73.6M/73.6M [00:00<00:00, 78.6MB/s]\n",
      "Downloading data: 100%|██████████| 81.1M/81.1M [00:00<00:00, 82.9MB/s]\n",
      "Downloading data: 100%|██████████| 77.6M/77.6M [00:01<00:00, 67.1MB/s]\n",
      "Downloading data: 100%|██████████| 74.0M/74.0M [00:01<00:00, 67.1MB/s]\n",
      "Downloading data: 100%|██████████| 74.9M/74.9M [00:00<00:00, 80.4MB/s]\n",
      "Downloading data: 100%|██████████| 75.2M/75.2M [00:00<00:00, 78.5MB/s]\n",
      "Downloading data: 100%|██████████| 73.5M/73.5M [00:01<00:00, 63.7MB/s]\n",
      "Downloading data: 100%|██████████| 74.7M/74.7M [00:01<00:00, 69.6MB/s]\n",
      "Downloading data: 100%|██████████| 79.0M/79.0M [00:01<00:00, 72.2MB/s]\n",
      "Downloading data: 100%|██████████| 82.3M/82.3M [00:00<00:00, 87.8MB/s]\n",
      "Downloading data: 100%|██████████| 74.3M/74.3M [00:01<00:00, 69.3MB/s]\n",
      "Downloading data: 100%|██████████| 60.7M/60.7M [00:01<00:00, 56.1MB/s]\n",
      "Downloading data: 100%|██████████| 66.7M/66.7M [00:01<00:00, 66.5MB/s]\n",
      "Downloading data: 100%|██████████| 53.9M/53.9M [00:00<00:00, 62.5MB/s]\n",
      "Downloading data: 100%|██████████| 64.4M/64.4M [00:00<00:00, 78.5MB/s]\n",
      "Downloading data: 100%|██████████| 60.0M/60.0M [00:00<00:00, 79.6MB/s]\n",
      "Downloading data: 100%|██████████| 59.2M/59.2M [00:00<00:00, 73.1MB/s]\n",
      "Generating train split: 1267441 examples [00:44, 28621.72 examples/s]\n",
      "Generating validation split: 130849 examples [00:04, 26275.12 examples/s]\n",
      "Generating test split: 143080 examples [00:04, 29798.38 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['contract_name', 'file_path', 'contract_address', 'language', 'class_name', 'class_code', 'class_documentation', 'class_documentation_type', 'func_name', 'func_code', 'func_documentation', 'func_documentation_type', 'compiler_version', 'license_type', 'swarm_source', 'meta', '__index_level_0__'],\n",
      "        num_rows: 1267441\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['contract_name', 'file_path', 'contract_address', 'language', 'class_name', 'class_code', 'class_documentation', 'class_documentation_type', 'func_name', 'func_code', 'func_documentation', 'func_documentation_type', 'compiler_version', 'license_type', 'swarm_source', 'meta', '__index_level_0__'],\n",
      "        num_rows: 130849\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['contract_name', 'file_path', 'contract_address', 'language', 'class_name', 'class_code', 'class_documentation', 'class_documentation_type', 'func_name', 'func_code', 'func_documentation', 'func_documentation_type', 'compiler_version', 'license_type', 'swarm_source', 'meta', '__index_level_0__'],\n",
      "        num_rows: 143080\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"andstor/smart_contract_code_comments\", 'default')\n",
    "\n",
    "# Print basic information about the dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Full Code and Code Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_cols = ['file_path', 'contract_address', 'language', 'class_name', 'class_documentation_type', 'func_name', 'func_code', 'func_documentation', 'func_documentation_type', 'compiler_version', 'license_type', 'swarm_source', 'meta', '__index_level_0__']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = train.remove_columns(remove_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "full_code = pd.DataFrame(code)\n",
    "full_code.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Headers and Function Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_func_col = ['file_path', 'contract_address', 'class_code', 'class_documentation', 'language', 'class_name', 'class_documentation_type', 'func_documentation_type', 'compiler_version', 'license_type', 'swarm_source', 'meta', '__index_level_0__']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_code = train.remove_columns(remove_func_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = pd.DataFrame(func_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define the function that extracts function headers\n",
    "def extract_function_header(func_code):\n",
    "    # Regular expression to match Solidity function declarations\n",
    "    func_header_regex = r'function\\s*[\\w\\d_]*\\s*\\([^)]*\\)\\s*[^{]*'\n",
    "    # Find all matches in the func_code\n",
    "    matches = re.findall(func_header_regex, func_code)\n",
    "    # Assume each func_code contains one function and take the first match\n",
    "    # If there are multiple matches, this will need adjustment\n",
    "    return matches[0] if matches else None\n",
    "\n",
    "# Assume 'data' is your DataFrame loaded from CSV and it has a column 'func_code'\n",
    "# You can apply the function directly on the DataFrame column\n",
    "func['func_header'] = func['func_code'].apply(extract_function_header)\n",
    "\n",
    "# filter out rows with no function headers\n",
    "func = func[func['func_header'].apply(lambda x: x is not None and len(x) > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = func.groupby('contract_name').agg({\n",
    "    'func_header': list,\n",
    "    'func_documentation': list\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      contract_name                                        func_header  \\\n",
      "0        $$$$$Token  [function totalSupply() public constant return...   \n",
      "1               $33  [function balanceOf(address account) external ...   \n",
      "2             $4RFI  [function totalSupply() external view returns ...   \n",
      "3              $AVG  [function totalSupply() public constant return...   \n",
      "4          $BANKINU  [function name() external view returns (string...   \n",
      "...             ...                                                ...   \n",
      "60688   zombittoken  [function totalSupply() constant returns (uint...   \n",
      "60689     zp_minter  [function supportsInterface(bytes4 interfaceId...   \n",
      "60690  zrankadictos  [function TokenERC20(\\r\\n    uint256 initialSu...   \n",
      "60691       zsToken  [function deposit(uint256 amount, uint256 _tok...   \n",
      "60692     zukiducks  [function tokenURI(uint256 _tokenId)\\r\\n    pu...   \n",
      "\n",
      "                                      func_documentation  \n",
      "0      [// ------------------------------------------...  \n",
      "1      [/**\\r\\n * @dev Returns the amount of tokens o...  \n",
      "2      [/**\\r\\n * @dev Returns the amount of tokens i...  \n",
      "3      [// ------------------------------------------...  \n",
      "4      [/**\\r\\n * @dev Returns the name of the token....  \n",
      "...                                                  ...  \n",
      "60688  [/// @return total amount of tokens, /// @para...  \n",
      "60689  [/**\\r\\n * @dev Returns true if this contract ...  \n",
      "60690  [/**\\r\\n * Constructor function\\r\\n *\\r\\n * In...  \n",
      "60691  [// Now handle deposits into the strategy, // ...  \n",
      "60692                                      [// Metadata]  \n",
      "\n",
      "[60693 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(grouped_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join On Contract Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = pd.merge(grouped_data, full_code, on='contract_name', how='left')\n",
    "cleaned_data = cleaned_data.drop_duplicates(subset=['contract_name'], keep='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contract_name                                                 $$$$$Token\n",
      "func_header            [function totalSupply() public constant return...\n",
      "func_documentation     [// ------------------------------------------...\n",
      "class_code             contract $$$$$Token is ERC20Interface, SafeMat...\n",
      "class_documentation    /**\\r\\n ERC20 Token, with the addition of symb...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_data.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Prompts into JSON File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format:\n",
    "- Task Description: `class_documentation` - contract comment\n",
    "- Key Functions: \n",
    "  - `func_header, func_documentation`: function header and comment\n",
    "- Constraints: The contract must be gas-efficient and include security checks to prevent common vulnerabilities (e.g., reentrancy, overflow/underflow).\n",
    "\n",
    "Example:\n",
    "\n",
    "- Task Description: Create a token according to the ERC-20 standard with a mechanism for freezing assets for a specified period.\n",
    "- Key Functions: \n",
    "  - `mint(address recipient, uint256 amount)`: Mints tokens to a specified address.\n",
    "  - `freeze(address target, uint256 untilTimestamp)`: Freezes the assets of a specified address until a certain timestamp.\n",
    "  - `transfer(address recipient, uint256 amount)`: Allows token transfers, checking if the sender's assets are frozen.\n",
    "- Constraints: The contract must be gas-efficient and include security checks to prevent common vulnerabilities (e.g., reentrancy, overflow/underflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 100 examples to use (TEST WITH 10)\n",
    "raw = cleaned_data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_user_prompt(class_documentation, func_code_list, func_comment_list):\n",
    "    prompt = 'Generate a smart contract using Solidity 0.4.17 that accomplishes the following requirements:\\n'\n",
    "    \n",
    "    prompt += 'Task Description: {}\\n'.format(class_documentation.replace(\"/\", \"\").replace(\"*\", \"\"))\n",
    "    prompt += 'Key Functions:\\n'\n",
    "    \n",
    "    for idx in range(len(func_code_list)):\n",
    "        comment =  func_comment_list[idx].replace(\"/\", \"\").replace(\"-\", \"\").replace(\"\\n\", \"\").replace(\"@notice\", \"\").replace(\"@param\", \"Parameter: \").replace(\"*\", \"\")\n",
    "        prompt += '{}: {}\\n'.format(func_code_list[idx], comment)\n",
    "    \n",
    "    prompt += 'Constraints: The contract must be gas-efficient and include security checks to prevent common vulnerabilities (e.g., reentrancy, overflow/underflow).\\n'\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "\n",
    "for idx, row in raw.iterrows():\n",
    "    prompt = [{\"role\": \"system\", \"content\": \"DEFI is an experienced Ethereum developer that writes properly functioning Solidity contracts.\"}]\n",
    "    prompt.append({\"role\":\"user\",\"content\":\"{}\".format(write_user_prompt(row['class_documentation'], row['func_header'], row['func_documentation']))})\n",
    "    prompt.append({\"role\": \"assistant\", \"content\": \"{}\".format(row['class_code'])})\n",
    "        \n",
    "    output.append({\"messages\":prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test-prompt.txt', 'w') as writefile:\n",
    "    writefile.write(output[0]['messages'][1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('3.json', 'w') as f:\n",
    "    json.dump(output[3], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "with jsonlines.open('prompts_final.jsonl', mode='w') as writer:\n",
    "    writer.write_all(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

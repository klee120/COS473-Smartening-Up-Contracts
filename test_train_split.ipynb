{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets transformers jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/klee12/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /users/klee12/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import interpreter_login\n",
    "\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Downloading readme: 100%|██████████| 1.15k/1.15k [00:00<00:00, 5.39MB/s]\n",
      "Downloading data: 100%|██████████| 79.0M/79.0M [00:01<00:00, 45.7MB/s]\n",
      "Downloading data: 100%|██████████| 80.0M/80.0M [00:01<00:00, 61.9MB/s]\n",
      "Downloading data: 100%|██████████| 78.6M/78.6M [00:01<00:00, 52.2MB/s]\n",
      "Downloading data: 100%|██████████| 73.6M/73.6M [00:01<00:00, 50.6MB/s]\n",
      "Downloading data: 100%|██████████| 79.6M/79.6M [00:01<00:00, 51.6MB/s]\n",
      "Downloading data: 100%|██████████| 77.5M/77.5M [00:01<00:00, 53.4MB/s]\n",
      "Downloading data: 100%|██████████| 79.1M/79.1M [00:01<00:00, 57.7MB/s]\n",
      "Downloading data: 100%|██████████| 83.1M/83.1M [00:01<00:00, 58.4MB/s]\n",
      "Downloading data: 100%|██████████| 77.7M/77.7M [00:01<00:00, 52.6MB/s]\n",
      "Downloading data: 100%|██████████| 82.5M/82.5M [00:01<00:00, 55.0MB/s]\n",
      "Downloading data: 100%|██████████| 73.6M/73.6M [00:01<00:00, 54.8MB/s]\n",
      "Downloading data: 100%|██████████| 81.1M/81.1M [00:01<00:00, 49.6MB/s]\n",
      "Downloading data: 100%|██████████| 77.6M/77.6M [00:01<00:00, 59.3MB/s]\n",
      "Downloading data: 100%|██████████| 74.0M/74.0M [00:01<00:00, 61.7MB/s]\n",
      "Downloading data: 100%|██████████| 74.9M/74.9M [00:01<00:00, 50.4MB/s]\n",
      "Downloading data: 100%|██████████| 75.2M/75.2M [00:01<00:00, 52.5MB/s]\n",
      "Downloading data: 100%|██████████| 73.5M/73.5M [00:01<00:00, 47.0MB/s]\n",
      "Downloading data: 100%|██████████| 74.7M/74.7M [00:01<00:00, 55.2MB/s]\n",
      "Downloading data: 100%|██████████| 79.0M/79.0M [00:01<00:00, 56.5MB/s]\n",
      "Downloading data: 100%|██████████| 82.3M/82.3M [00:01<00:00, 53.8MB/s]\n",
      "Downloading data: 100%|██████████| 74.3M/74.3M [00:01<00:00, 42.6MB/s]\n",
      "Downloading data: 100%|██████████| 60.7M/60.7M [00:01<00:00, 51.8MB/s]\n",
      "Downloading data: 100%|██████████| 66.7M/66.7M [00:01<00:00, 52.3MB/s]\n",
      "Downloading data: 100%|██████████| 53.9M/53.9M [00:01<00:00, 51.7MB/s]\n",
      "Downloading data: 100%|██████████| 64.4M/64.4M [00:01<00:00, 51.3MB/s]\n",
      "Downloading data: 100%|██████████| 60.0M/60.0M [00:01<00:00, 55.1MB/s]\n",
      "Downloading data: 100%|██████████| 59.2M/59.2M [00:01<00:00, 49.2MB/s]\n",
      "Generating train split: 1267441 examples [00:38, 32770.11 examples/s]\n",
      "Generating validation split: 130849 examples [00:04, 29320.21 examples/s]\n",
      "Generating test split: 143080 examples [00:04, 29848.65 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['contract_name', 'file_path', 'contract_address', 'language', 'class_name', 'class_code', 'class_documentation', 'class_documentation_type', 'func_name', 'func_code', 'func_documentation', 'func_documentation_type', 'compiler_version', 'license_type', 'swarm_source', 'meta', '__index_level_0__'],\n",
      "        num_rows: 1267441\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['contract_name', 'file_path', 'contract_address', 'language', 'class_name', 'class_code', 'class_documentation', 'class_documentation_type', 'func_name', 'func_code', 'func_documentation', 'func_documentation_type', 'compiler_version', 'license_type', 'swarm_source', 'meta', '__index_level_0__'],\n",
      "        num_rows: 130849\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['contract_name', 'file_path', 'contract_address', 'language', 'class_name', 'class_code', 'class_documentation', 'class_documentation_type', 'func_name', 'func_code', 'func_documentation', 'func_documentation_type', 'compiler_version', 'license_type', 'swarm_source', 'meta', '__index_level_0__'],\n",
      "        num_rows: 143080\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"andstor/smart_contract_code_comments\", 'default')\n",
    "\n",
    "# Print basic information about the dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_code(dataset):\n",
    "    remove_cols = ['file_path', 'contract_address', 'language', 'class_name', 'class_documentation_type', 'func_name', 'func_code', 'func_documentation', 'func_documentation_type', 'compiler_version', 'license_type', 'swarm_source', 'meta', '__index_level_0__']\n",
    "    code = dataset.remove_columns(remove_cols)\n",
    "    \n",
    "    full_code = pd.DataFrame(code)\n",
    "    full_code.drop_duplicates(inplace=True)\n",
    "    \n",
    "    return full_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define the function that extracts function headers\n",
    "def extract_function_header(func_code):\n",
    "    # Regular expression to match Solidity function declarations\n",
    "    func_header_regex = r'function\\s*[\\w\\d_]*\\s*\\([^)]*\\)\\s*[^{]*'\n",
    "    # Find all matches in the func_code\n",
    "    matches = re.findall(func_header_regex, func_code)\n",
    "    # Assume each func_code contains one function and take the first match\n",
    "    # If there are multiple matches, this will need adjustment\n",
    "    return matches[0] if matches else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_func(dataset):\n",
    "    remove_func_col = ['file_path', 'contract_address', 'class_code', 'class_documentation', 'language', 'class_name', 'class_documentation_type', 'func_documentation_type', 'compiler_version', 'license_type', 'swarm_source', 'meta', '__index_level_0__']\n",
    "    func_code = dataset.remove_columns(remove_func_col)\n",
    "    \n",
    "    func = pd.DataFrame(func_code)\n",
    "    \n",
    "    func['func_header'] = func['func_code'].apply(extract_function_header)\n",
    "\n",
    "    # filter out rows with no function headers\n",
    "    func = func[func['func_header'].apply(lambda x: x is not None and len(x) > 0)]\n",
    "    \n",
    "    # group by contract\n",
    "    grouped = func.groupby('contract_name').agg({\n",
    "        'func_header': list,\n",
    "        'func_documentation': list\n",
    "    }).reset_index()\n",
    "    \n",
    "    return grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset['train']\n",
    "train_code = clean_code(train)\n",
    "train_func = clean_func(train)\n",
    "\n",
    "train_clean = pd.merge(train_code, train_func, on='contract_name', how='left')\n",
    "train_clean = train_clean.drop_duplicates(subset=['contract_name'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_filter = train_clean[~train_clean['class_code'].str.startswith('interface')]\n",
    "subset_filter = subset_filter[~subset_filter['class_code'].str.startswith('contract Token')]\n",
    "subset_filter = subset_filter[~subset_filter['class_code'].str.startswith('contract Owned')]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
